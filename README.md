# Приложение для оценки пользовательского взаимодействия с применением трекинга взгляда, анализа мимики и расчёта индивидуального эффекта воздействия

## Системные требования

Для корректной работы приложения необходимо установить следующие компоненты
- **Python** версии `3.12.7`
- **Node.js** и менеджер пакетов `npm` (или `yarn`)

## Установка и запуск
1. **Клонируйте репозиторий**
    ```bash
    git clone <URL-репозитория>
    ```
2. **Установите зависимости Python**
    ```bash
    pip install -r requirements.txt
    ```
3. **Установите зависимости Node.js**
    ```bash
    cd app
    npm install
    ```
4. **Запустите backend сервер**
    ```bash
    python app.py
    ```
    Сервер будет доступен по адресу: [`http://localhost:5000`](http://localhost:5000)

5. **Откройте интерфейс**
    Перейдите по ссылке [`http://127.0.0.1:5000/participant.html`](http://127.0.0.1:5000/participant.html) в браузере **Chrome** или **Firefox**.

---

## Проведение эксперимента

### 1. Инициализация участника
При первом запуске `participant.html` или при отсутствии данных в локальном хранилище браузера откроется форма ввода информации об участнике
- введите уникальный идентификатор участника и отметьте согласия;
- после нажатия кнопки **"Готово"** произойдёт переход на основную страницу (`index.html`).

> ⚠️ Для каждого нового участника (или при смене браузера) необходимо повторно пройти эту процедуру.

---

### 2. Основной интерфейс
На странице `index.html` представлены
- панель управления;
- видео с камеры (для анализа мимики и трекинга взгляда);
- область Canvas для визуализации gaze-точек;
- модальные окна с заданиями и результатами анализа.

> **Важно:** дождитесь полной загрузки WebGazer. В браузере Firefox это может занять больше времени. Начинать эксперимент следует **только после появления изображения с камеры и точек на лице**.

---

### 3. Калибровка трекера взгляда
Перед выполнением заданий необходимо откалибровать систему:

1. Нажмите **"Калибровка"**.
2. В появившемся окне нажмите поочерёдно **по 5 раз** на каждую из 9 точек на экране.
3. Во время калибровки **избегайте движения головы**. Квадрат в окне камеры должен оставаться **зелёным**.
4. Далее будет предложено сфокусироваться на центральной точке экрана в течение **5 секунд** — это необходимо для определения точности модели.
5. После завершения отобразится окно с оценкой точности. При необходимости можно повторить калибровку.

---

### 4. Выполнение экспериментальных заданий
1. Нажмите кнопку **"Начать"**. Включится секундомер, начнётся запись данных.
2. Перейдите в **панель заданий**, выберите задание и нажмите **"Начать"**.
3. Выполните задание в интерфейсе браузера.
4. После завершения задания нажмите **"Завершить"**.
5. Повторите процесс для **трёх различных заданий**.
6. По окончании эксперимента нажмите кнопку **"Завершить"** в основной панели.

Во время эксперимента автоматически фиксируются:
- данные трекинга взгляда (WebGazer),
- выражения лица (FaceAPI),
- пользовательские взаимодействия (клики, прокрутка и др.).

---

### 5. Повтор на другом браузере
После завершения эксперимента в одном браузере (например, Chrome) необходимо **повторить эксперимент в другом** (например, Firefox) с теми же действиями.
Эксперименты проводились с использованием ноутбуков Lenovo Ideapad S340-14IWL и AcerBlack.
---

## Рекомендации
- Обеспечьте **хорошее освещение** и стабильную позицию головы участника.
- Не закрывайте и не обновляйте страницу в процессе эксперимента.
- Собранные данные хранятся локально и могут быть экспортированы для дальнейшего анализа индивидуального эффекта воздействия.

# About
The repository presents a framework for investigation individual effects of UI treatment.

The project is completed as Magister Thesis project by Ivan P. Eroshin at SPbPU Institute of Computer Science and Cybersecurity (SPbPU ICSC).

Persons
The main contributor of the project is Ivan P. Eroshin, a student of SPbPU ICSC.
The advisor and minor contributor is Vladimir A. Parkhomenko a seniour lecturer of SPbPU ICSC. 

Warranty
The contributors give no warranty for the using of the software. The authors kindly asks to cite paper which will be published soon [1].

License
This program is open to use anywhere and is licensed under the MIT license.

[1] User Interface Evaluation Using Tracking Eyes and Facial Expressions, DAMDID 2025
